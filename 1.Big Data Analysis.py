# -*- coding: utf-8 -*-
"""DA-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJjUFH4uEi8lWxSQ2DFuzjxRCG6GHO5E
"""

# DOWNLOAD DATASET

import kagglehub

# Download latest version
path = kagglehub.dataset_download("imrulhasanrobi/e-commerce-big-dataset-from-multi-category")

print("Path to dataset files:", path)

# INITIALISE SPARK


from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Ecommerce Big Data Analysis") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

spark

#LOAD DATASET

from pyspark.sql.functions import *

# Replace with actual CSV filename inside dataset folder
df = spark.read.csv(f"{path}/*.csv",
                    header=True,
                    inferSchema=True)

df.printSchema()
df.show(5)

# DATA CLEANING

# Remove duplicates
df = df.dropDuplicates()

# Cache for performance (since reused multiple times)
df.cache()

# TOTAL RECORDS CHECK

print("Total Records:", df.count())

from pyspark.sql.functions import sum, count, desc

# Top 10 categories by sales
top_categories = df.filter(df.current_price.isNotNull()) \
    .groupBy("productType") \
    .agg(
        count("*").alias("total_transactions"),   # number of products sold/listed
        sum("current_price").alias("total_sales") # total revenue
    ) \
    .orderBy(desc("total_sales")) \
    .limit(10)

top_categories.show(truncate=False)

# Top Brands by Revenue

from pyspark.sql.functions import sum, desc

top_brands = df.groupBy("brand_name") \
    .agg(sum("current_price").alias("brand_revenue")) \
    .orderBy(desc("brand_revenue")) \
    .limit(10)

top_brands.show()

# Product/Brand Purchase Behavior (instead of user)
from pyspark.sql.functions import sum, count, desc

# Top 10 products by total revenue
product_activity = df.filter(df.current_price.isNotNull()) \
    .groupBy("product_id", "title") \
    .agg(
        count("*").alias("total_listings"),
        sum("current_price").alias("total_revenue")
    ) \
    .orderBy(desc("total_revenue")) \
    .limit(10)

product_activity.show(truncate=False)

# Brand-Level Aggregation (alternative to user-level)

brand_activity = df.filter(df.current_price.isNotNull()) \
    .groupBy("brand_name") \
    .agg(
        count("*").alias("total_products"),
        sum("current_price").alias("total_revenue")
    ) \
    .orderBy(desc("total_revenue")) \
    .limit(10)

brand_activity.show(truncate=False)

# Monthly Revenue Trend

from pyspark.sql.functions import sum, desc

monthly_sales_proxy = df.filter(df.current_price.isNotNull()) \
    .groupBy("productType") \
    .agg(sum("current_price").alias("total_revenue")) \
    .orderBy(desc("total_revenue"))

monthly_sales_proxy.show(truncate=False)

# Conversion Funnel

from pyspark.sql.functions import col

# Compute discount percentage
df_discount = df.withColumn(
    "discount_pct",
    ((col("rrp") - col("current_price")) / col("rrp")) * 100
)

df_discount.select("product_id", "title", "current_price", "rrp", "discount_pct") \
           .orderBy(desc("discount_pct")) \
           .show(10, truncate=False)

# Performance & Scalability Demonstration
# Check partitions
print("Number of partitions:", df.rdd.getNumPartitions())

# Repartition by productType (category) for better parallelism
df = df.repartition(300, "productType")
print("New partitions:", df.rdd.getNumPartitions())

# Persist in memory for repeated queries
df.persist()

# Advanced Big Data Analysis — Product Clustering (instead of user clustering)


from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

# Features: current_price and discount_pct
df_cluster = df_discount.filter(col("current_price").isNotNull() & col("discount_pct").isNotNull())

assembler = VectorAssembler(
    inputCols=["current_price", "discount_pct"],
    outputCol="features"
)

final_data = assembler.transform(df_cluster)

# KMeans clustering
kmeans = KMeans(k=5, seed=1)
model = kmeans.fit(final_data)

clusters = model.transform(final_data)
clusters.select("product_id", "title", "current_price", "discount_pct", "prediction") \
        .show(10, truncate=False)

# 1️⃣ Setup
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pyspark.sql.functions import col, sum, desc
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

# Optional styling
sns.set(style="whitegrid")

# -----------------------------
# 2️⃣ Top 10 Brands by Revenue
# -----------------------------
top_brands = df.filter(df.current_price.isNotNull()) \
    .groupBy("brand_name") \
    .agg(sum("current_price").alias("total_revenue")) \
    .orderBy(desc("total_revenue")) \
    .limit(10)

top_brands_pd = top_brands.toPandas()

plt.figure(figsize=(12,6))
sns.barplot(data=top_brands_pd, x="total_revenue", y="brand_name", palette="viridis")
plt.title("Top 10 Brands by Revenue")
plt.xlabel("Total Revenue (USD)")
plt.ylabel("Brand Name")
plt.show()

# -----------------------------
# 3️⃣ Top 10 Categories by Revenue
# -----------------------------
top_categories = df.filter(df.current_price.isNotNull()) \
    .groupBy("productType") \
    .agg(sum("current_price").alias("total_revenue")) \
    .orderBy(desc("total_revenue")) \
    .limit(10)

top_categories_pd = top_categories.toPandas()

plt.figure(figsize=(12,6))
sns.barplot(data=top_categories_pd, x="total_revenue", y="productType", palette="coolwarm")
plt.title("Top 10 Product Categories by Revenue")
plt.xlabel("Total Revenue (USD)")
plt.ylabel("Category")
plt.show()

# -----------------------------
# 4️⃣ Discount Distribution
# -----------------------------
df_discount = df.withColumn(
    "discount_pct",
    ((col("rrp") - col("current_price")) / col("rrp")) * 100
)

# Sample for visualization
discount_sample = df_discount.select("discount_pct").dropna().sample(False, 0.1).toPandas()

plt.figure(figsize=(10,6))
sns.histplot(discount_sample["discount_pct"], bins=30, kde=True, color="skyblue")
plt.title("Discount Percentage Distribution")
plt.xlabel("Discount %")
plt.ylabel("Number of Products")
plt.show()

# -----------------------------
# 5️⃣ Price vs Discount Clustering (KMeans)
# -----------------------------
df_cluster = df_discount.filter(col("current_price").isNotNull() & col("discount_pct").isNotNull())

assembler = VectorAssembler(inputCols=["current_price", "discount_pct"], outputCol="features")
final_data = assembler.transform(df_cluster)

kmeans = KMeans(k=5, seed=1)
model = kmeans.fit(final_data)
clusters = model.transform(final_data)

clusters_pd = clusters.select("current_price", "discount_pct", "prediction").toPandas()

plt.figure(figsize=(10,6))
sns.scatterplot(
    data=clusters_pd,
    x="current_price",
    y="discount_pct",
    hue="prediction",
    palette="tab10",
    s=60
)
plt.title("Product Clusters by Price & Discount")
plt.xlabel("Current Price (USD)")
plt.ylabel("Discount %")
plt.legend(title="Cluster")
plt.show()

# -----------------------------
# 6️⃣ Price Distribution by Category (Optional)
# -----------------------------
sample_df = df.filter(df.current_price.isNotNull()).sample(False, 0.1).toPandas()

plt.figure(figsize=(12,6))
sns.boxplot(x="productType", y="current_price", data=sample_df)
plt.xticks(rotation=45)
plt.title("Price Distribution by Product Category")
plt.xlabel("Category")
plt.ylabel("Current Price (USD)")
plt.show()