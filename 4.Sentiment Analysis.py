# -*- coding: utf-8 -*-
"""DA-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJjUFH4uEi8lWxSQ2DFuzjxRCG6GHO5E
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("abhi8923shriv/sentiment-analysis-dataset")

print("Path to dataset files:", path)

# Install necessary libraries (if not already installed)
!pip install pandas numpy matplotlib seaborn scikit-learn nltk wordcloud transformers torch

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Text Cleaning & Preprocessing

# Ensure NLTK stopwords are downloaded
nltk.download('stopwords')

# Strip whitespace from column names to handle potential hidden characters
df.columns = df.columns.str.strip()

# Rename columns for easier access
df.rename(columns={'polarity of tweet': 'sentiment', 'text of the tweet': 'text'}, inplace=True)

# Look at basic info to confirm column names
df.info()

# Create a clean_text column removing URLs, punctuation, numbers, and stopwords
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)  # remove URLs
    text = re.sub(r"[^a-z\s]", "", text)         # keep text only
    text = " ".join(word for word in text.split() if word not in stop_words)
    return text

df['clean_text'] = df['text'].apply(clean_text)
df[['text','clean_text','sentiment']].head()

# Visualize Sentiment Distribution

sns.countplot(x='sentiment', data=df)
plt.title("Sentiment Distribution")
plt.show()

# Optional: WordCloud for positive sentiment
positive_text = " ".join(df[df.sentiment==4]['clean_text']) # Sentiment 4 corresponds to 'positive' in this dataset
wordcloud = WordCloud(width=800, height=400).generate(positive_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Positive Words Wordcloud")
plt.show()

#Feature Extraction + Train/Test Split

tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['clean_text'])
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Train Naive Bayes Classifier

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()